{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff177d81",
   "metadata": {},
   "source": [
    "# Cosmological structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc7aa419",
   "metadata": {},
   "source": [
    "Here, we use `twinLab` to create an emulator for the statistical properties of the distribution of structure across the Universe.\n",
    "\n",
    "The large-scale distribution of billions of galaxies contains a wealth of information about the origin, expansion, and contents of the Universe. For example, the galaxy distribution is sensitive to the amounts of dark energy and dark matter, the current and historial expansion speed, and the process of inflation that occurred soon after the big bang and which seeded the diverse array of cosmological structure, including all galaxies, stars and planets, that we see today. Galaxies exist in dense clumps, called groups, clusters, super clusters, depending on the number, at the nodes of the density distribution. The underlying density is governed by the dynamics and properties of dark matter, which defines a skeleton along which galaxies flow and eventually cluster.\n",
    "\n",
    "To extract cosmological information from the galaxy distribution requires precise models of the statistical properties of the distribution as a function of the underlying parameters. Analytical theories, developed over the last 30 years, work at early times and on extremely large scales, where perturbations to the mean density are small. However, on the (comparatively small) scale of galaxies, the perturbations are huge and modelling their distribution can only be accurately achieved using expensive $n$-body simulations (the image above shows a slice of density through one such simulation). High-fidelity simulations can take up to a month to run distributed across tens of thousands of cores on the top super computers in the world. It is impractical to run accurate simulations at all points in parameter space, especially since the parameter-space of models under investigation is always expanding. In modern cosmology this includes the space of exotic dark energy models, beyond-Einstein gravity theories, and non-standard particle physics models for dark matter and neutrinos.\n",
    "\n",
    "In this example, we use `twinLab` to create an emulator for the matter power spectrum, a statistical quantity that contains a subset of the possible information from the clustering distribution of galaxies. The power spectrum can both be computed via simulation and measured in observational datasets. The training of the `twinLab` emulator is performed online (in the cloud) and is completed in a matter of minutes. Once trained, the emulator can be used for extremely rapid power-spectrum evaluation across parameter space in a way that interpolates and extrapolates reasonably. The major benefit of using `twinLab` is that we get an accurate estimate of our model uncertainty for free, so that we know exactly how much we should trust our trained surrogate. The model is trained on approximate simulation data that occupy a Latin-hypercube distribution across five parameters of interest to cosmologists. The model can be rapidly retrained if necessary, and additional parameters can be incorporated if desired."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "713fe7a1",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Import the libraries and the `twinLab` client with. Note that We need to supply our credentials to use `twinLab`, and these should be in a `.env` file in the project root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# twinLab\n",
    "import twinlab as tl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6942ac1f",
   "metadata": {},
   "source": [
    "Ensure that the correct group and user names are reported, these are taken from your `.env` file, which should be present in the root directory of the repository (you can construct this from `.env.example`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a73e0a",
   "metadata": {},
   "source": [
    "Set some parameters for training the machine-learning campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Campaign\n",
    "CAMPAIGN_ID = \"cosmology\"\n",
    "\n",
    "# Data options\n",
    "POWER_RATIO = True\n",
    "POWER_LOG = True\n",
    "POWER_LATIN = True\n",
    "TRAINING_POINTS = 100\n",
    "SVD_VARIANCE = 0.9999\n",
    "\n",
    "# Data files\n",
    "TRAINING_FILEBASE = \"cosmo\"\n",
    "EVALUATION_FILEBASE = \"eval\"\n",
    "GRID_DATA = \"grid.csv\"\n",
    "\n",
    "# Directories\n",
    "CAMPAIGN_DIR = \"./resources/campaigns/cosmology\"\n",
    "DATASETS_DIR = \"./resources/datasets\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f5aa377",
   "metadata": {},
   "source": [
    "Do some minor calculations to sort the file names out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suffixes for data files\n",
    "latin_thing = '_latin' if POWER_LATIN else ''\n",
    "ratio_thing = '_ratio' if POWER_RATIO else ''\n",
    "log_thing = '_log' if POWER_LOG else ''\n",
    "\n",
    "# Construct training and evaluation data file names\n",
    "TRAINING_DATA = TRAINING_FILEBASE+latin_thing+ratio_thing+log_thing+\".csv\"\n",
    "EVALUATION_DATA = EVALUATION_FILEBASE+ratio_thing+log_thing+\".csv\"\n",
    "\n",
    "# File paths\n",
    "DATASET_PATH = os.path.join(DATASETS_DIR, TRAINING_DATA)\n",
    "EVALUATION_PATH = os.path.join(CAMPAIGN_DIR, EVALUATION_DATA)\n",
    "GRID_PATH = os.path.join(CAMPAIGN_DIR, GRID_DATA)\n",
    "\n",
    "# Write to screen\n",
    "print(f\"Grid........ {GRID_PATH}\")\n",
    "print(f\"Dataset..... {DATASET_PATH}\")\n",
    "print(f\"Evaluate.... {EVALUATION_PATH}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8fca977",
   "metadata": {},
   "source": [
    "Upload the training dataset to the cloud for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a059f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.upload_dataset(DATASET_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9ff52fa",
   "metadata": {},
   "source": [
    "List which datasets are available to train with (and check that our dataset uploaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tl.list_datasets()\n",
    "pprint(datasets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9973bdd",
   "metadata": {},
   "source": [
    "You can query the freshly-uploaded dataset to provide a statistical summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1dc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tl.query_dataset(TRAINING_DATA)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d02ebd9a",
   "metadata": {},
   "source": [
    "Set the emulator training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmological_parameters = [\"Omega_c\", \"Omega_b\", \"h\", \"ns\", \"w0\"] # We will take these cosmological parameters as input\n",
    "nk = 100\n",
    "wavenumber_columns = [f\"k{i}\" for i in range(nk)] # We will try to learn P(k) at these wavenumbers\n",
    "params = { # Parameters file for twinLab\n",
    "    \"filename\": TRAINING_DATA,\n",
    "    \"inputs\": cosmological_parameters,\n",
    "    \"outputs\": wavenumber_columns,\n",
    "    \"decompose_outputs\": True,\n",
    "    \"output_explained_variance\": SVD_VARIANCE,\n",
    "    \"train_test_split\": TRAINING_POINTS,\n",
    "}\n",
    "pprint(params, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b083f8",
   "metadata": {},
   "source": [
    "Train the emulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.train_campaign(params, CAMPAIGN_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c75b9857",
   "metadata": {},
   "source": [
    "Check which campaigns are trained with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = tl.list_campaigns()\n",
    "pprint(campaigns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f359f0d9",
   "metadata": {},
   "source": [
    "View the metadata of the emulator with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3911b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tl.query_campaign(CAMPAIGN_ID)\n",
    "pprint(query, compact=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "106a9c99",
   "metadata": {},
   "source": [
    "We can now make predictions using trained emulator, this is now for previously unseen sets of cosmological parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_mean, df_eval_std = tl.predict_campaign(EVALUATION_PATH, CAMPAIGN_ID)\n",
    "df_train_mean, df_train_std = tl.predict_campaign(DATASET_PATH, CAMPAIGN_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c97b226",
   "metadata": {},
   "source": [
    "Read in the evaluation data and the grid of $k$ values on which to evaluate $P(k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATASET_PATH)\n",
    "df_grid = pd.read_csv(GRID_PATH)\n",
    "df_eval = pd.read_csv(EVALUATION_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed4e8974",
   "metadata": {},
   "source": [
    "Plot the accuracy of the trained emulator as a function of wavenumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy statistics\n",
    "if POWER_LOG: # Note that the difference is appropriate here because the data may be log\n",
    "    accuracy_eval = (df_eval_mean[wavenumber_columns]-df_eval[wavenumber_columns]).std()\n",
    "    accuracy_train = (df_train_mean[wavenumber_columns]-df_train[wavenumber_columns]).std()\n",
    "else:\n",
    "    accuracy_eval = (df_eval_mean[wavenumber_columns]/df_eval[wavenumber_columns]-1.).std()\n",
    "    accuracy_train = (df_train_mean[wavenumber_columns]/df_train[wavenumber_columns]-1.).std()\n",
    "\n",
    "plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "# Accuracy of model\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df_grid.iloc[0].values, 100.*accuracy_eval, label='Evaluation data')\n",
    "plt.plot(df_grid.iloc[0].values, 100.*accuracy_train, label='Training data')\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "plt.ylim(bottom=0.)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy of preicted error\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df_grid.iloc[0].values, accuracy_eval/df_eval_std.iloc[0].values)\n",
    "plt.plot(df_grid.iloc[0].values, accuracy_train/df_train_std.iloc[0].values)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"Fourier wavenumber [$h$ Mpc$^{-1}$]\")\n",
    "plt.axhline(1., color=\"k\", linestyle=\":\", lw=0.5)\n",
    "plt.ylabel(\"Relative accuracy\")\n",
    "plt.ylim((0., 2.))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04b455f2",
   "metadata": {},
   "source": [
    "Plot the raw performance of the emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff61305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "nsig = [1, 2]  # sigma errors to plot\n",
    "mfac = 1.  # Factor to inflate error bars by\n",
    "alpha_data = 0.5\n",
    "alpha_model = 0.5\n",
    "color_model = \"C0\"\n",
    "plot_band = True\n",
    "plot_mean = True\n",
    "npow = 5\n",
    "\n",
    "if mfac != 1.:\n",
    "    print(f\"Error magnification factor: {mfac} \\n\")\n",
    "\n",
    "# Plot power\n",
    "plt.subplots(1, npow, figsize=(npow*3.5, 3.), sharex=True, sharey=True)\n",
    "grid = df_grid.iloc[0].values\n",
    "for i in range(npow):\n",
    "    plt.subplot(1, npow, i+1)\n",
    "    if POWER_RATIO:\n",
    "        plt.axhline(1., color=\"black\", lw=0.5)\n",
    "    eval = df_eval[wavenumber_columns].iloc[i].values\n",
    "    mean = df_eval_mean.iloc[i].values\n",
    "    err = df_eval_std.iloc[i].values\n",
    "    if POWER_LOG:\n",
    "        eval, mean = np.exp(eval), np.exp(mean)\n",
    "    label = \"Evaluation data\" if i==0 else None\n",
    "    plt.plot(grid, eval, color=\"black\", alpha=alpha_data, label=label)\n",
    "    if plot_band:\n",
    "        for sig in nsig:\n",
    "            if POWER_LOG:\n",
    "                ymin, ymax = np.exp(-mfac*sig*err), np.exp(mfac*sig*err)\n",
    "                ymin, ymax = mean*ymin, mean*ymax\n",
    "            else:\n",
    "                ymin, ymax = -mfac*sig*err, mfac*sig*err\n",
    "                ymin, ymax = mean+ymin, mean+ymax\n",
    "            label = \"Model\" if sig==nsig[0] else None\n",
    "            plt.fill_between(grid, ymin, ymax, color=color_model, lw=0., alpha=alpha_model/sig, label=label)\n",
    "    if plot_mean:\n",
    "        label  = \"Model\" if not plot_band else None\n",
    "        plt.plot(grid, mean, color=color_model, alpha=alpha_model, label=label)\n",
    "    plt.xlabel(r\"Fourier wavenumber [$h$ Mpc$^{-1}$]\")\n",
    "    plt.xscale(\"log\")\n",
    "    if i==0: \n",
    "        if POWER_RATIO:\n",
    "            plt.ylabel(r\"Power ratio with linear\")\n",
    "        else:\n",
    "            plt.ylabel(r\"Power spectrum [$(h^{-1}\\mathrm{Mpc})^3$]\")\n",
    "        plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    if not POWER_RATIO:\n",
    "        plt.ylim((1e0, 1e5))\n",
    "    else:\n",
    "        plt.ylim((1., 100.))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "145b6038",
   "metadata": {},
   "source": [
    "And plot the residuals between model predictions and the truth for independent examples of training (seen by the model during training) and evaluation (unseen by the model) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nsig = [1, 2]\n",
    "alpha_data = 0.5\n",
    "alpha_model = 0.5\n",
    "color_model = \"C0\"\n",
    "plot_train = True\n",
    "alpha_train = 0.5\n",
    "color_train = \"C1\"\n",
    "dr = 3.\n",
    "plot_band = True\n",
    "plot_mean = True\n",
    "ncos = 25\n",
    "nrow = 5\n",
    "\n",
    "# Calculations\n",
    "ncol = ncos//nrow\n",
    "\n",
    "# Plot\n",
    "grid = df_grid.iloc[0].values\n",
    "plt.subplots(nrow, ncol, figsize=(3*ncol, 2.5*nrow), sharex=True, sharey=True)\n",
    "for i in range(ncos):\n",
    "    plt.axhline(0., color=\"black\", lw=1)\n",
    "    plt.subplot(nrow, ncol, i+1)\n",
    "    eval = df_eval[wavenumber_columns].iloc[i].values\n",
    "    eval_mean = df_eval_mean.iloc[i].values\n",
    "    eval_err = df_eval_std.iloc[i].values\n",
    "    train = df_train[wavenumber_columns].iloc[i].values\n",
    "    train_mean = df_train_mean.iloc[i].values\n",
    "    train_err = df_train_std.iloc[i].values\n",
    "    if POWER_LOG:\n",
    "        eval, eval_mean = np.exp(eval), np.exp(eval_mean)\n",
    "        train, train_mean = np.exp(train), np.exp(train_mean)\n",
    "    if plot_band:\n",
    "        for sig in nsig:\n",
    "            if POWER_LOG:\n",
    "                ymin, ymax = np.exp(-sig*eval_err), np.exp(sig*eval_err)\n",
    "                ymin, ymax = 100.*((eval_mean*ymin)/eval-1.), 100.*((eval_mean*ymax)/eval-1.)\n",
    "            else:\n",
    "                ymin, ymax = -sig*eval_err, sig*eval_err\n",
    "                ymin, ymax = 100.*((eval_mean+ymin)/eval-1.), 100.*((eval_mean+ymax)/eval-1.)\n",
    "            label = \"Model on evaluation data\" if sig==nsig[0] else None\n",
    "            plt.fill_between(grid, ymin, ymax, color=color_model, lw=0, alpha=alpha_model/sig, label=label)\n",
    "            if plot_train:\n",
    "                if POWER_LOG:\n",
    "                    ymin, ymax = np.exp(-sig*train_err), np.exp(sig*train_err)\n",
    "                    ymin, ymax = 100.*((train_mean*ymin)/train-1.), 100.*((train_mean*ymax)/train-1.)\n",
    "                else:\n",
    "                    ymin, ymax = -sig*train_err, sig*train_err\n",
    "                    ymin, ymax = 100.*((train_mean+ymin)/train-1.), 100.*((train_mean+ymax)/train-1.)\n",
    "                label = \"Model on training data\" if sig==nsig[0] else None\n",
    "                plt.fill_between(grid, ymin, ymax, color=color_train, lw=0, alpha=alpha_train/sig, label=label)\n",
    "    if plot_mean:\n",
    "        label = \"Model on evaluation data\" if not plot_band else None\n",
    "        y = 100.*(eval_mean/eval-1.)\n",
    "        plt.plot(grid, y, color=color_model, alpha=alpha_model, label=label)\n",
    "        if plot_train:\n",
    "            label = \"Model on training data\" if not plot_band else None\n",
    "            y = 100.*(train_mean/train-1.)\n",
    "            plt.plot(grid, y, color=color_train, alpha=alpha_train, label=label)\n",
    "    if i//ncol==nrow-1: plt.xlabel(r\"$k/h\\mathrm{Mpc}^{-1}$\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlim((grid[0], grid[-1]))\n",
    "    if i%ncol==0: plt.ylabel(r\"$P_\\mathrm{model}(k)/P_\\mathrm{truth}(k)-1$ [%]\")\n",
    "    plt.ylim((-dr, dr))\n",
    "    if i==0: plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc41b52",
   "metadata": {},
   "source": [
    "Finally, delete the trained emulator and dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.delete_campaign(CAMPAIGN_ID)\n",
    "tl.delete_dataset(TRAINING_DATA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
