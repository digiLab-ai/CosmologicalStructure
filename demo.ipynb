{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff177d81",
   "metadata": {},
   "source": [
    "# Cosmological structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc7aa419",
   "metadata": {},
   "source": [
    "Here, we use `twinLab` to create an emulator for the statistical properties of the distribution of structure across the Universe.\n",
    "\n",
    "The large-scale distribution of billions of galaxies contains a wealth of information about the origin, expansion, and contents of the Universe. For example, the galaxy distribution is sensitive to the amounts of dark energy and dark matter, the current and historial expansion speed, and the process of inflation that occurred soon after the big bang and which seeded the diverse array of cosmological structure, including all galaxies, stars and planets, that we see today. Galaxies exist in dense clumps, called groups, clusters, super clusters, depending on the number, at the nodes of the density distribution. The underlying density is governed by the dynamics and properties of dark matter, which defines a skeleton along which galaxies flow and eventually cluster.\n",
    "\n",
    "To extract cosmological information from the galaxy distribution requires precise models of the statistical properties of the distribution as a function of the underlying parameters. Analytical theories, developed over the last 30 years, work at early times and on extremely large scales, where perturbations to the mean density are small. However, on the (comparatively small) scale of galaxies, the perturbations are huge and modelling their distribution can only be accurately achieved using expensive $n$-body simulations (the image above shows a slice of density through one such simulation). High-fidelity simulations can take up to a month to run distributed across tens of thousands of cores on the top super computers in the world. It is impractical to run accurate simulations at all points in parameter space, especially since the parameter-space of models under investigation is always expanding. In modern cosmology this includes the space of exotic dark energy models, beyond-Einstein gravity theories, and non-standard particle physics models for dark matter and neutrinos.\n",
    "\n",
    "In this example, we use `twinLab` to create an emulator for the matter power spectrum, a statistical quantity that contains a subset of the possible information from the clustering distribution of galaxies. The power spectrum can both be computed via simulation and measured in observational datasets. The training of the `twinLab` emulator is performed online (in the cloud) and is completed in a matter of minutes. Once trained, the emulator can be used for extremely rapid power-spectrum evaluation across parameter space in a way that interpolates and extrapolates reasonably. The major benefit of using `twinLab` is that we get an accurate estimate of our model uncertainty for free, so that we know exactly how much we should trust our trained surrogate. The model is trained on approximate simulation data that occupy a Latin-hypercube distribution across five parameters of interest to cosmologists. The model can be rapidly retrained if necessary, and additional parameters can be incorporated if desired."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "713fe7a1",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Import th libraries and the `twinLab` client with. Note that We need to supply our credentials to use `twinLab`, and these should be in a `.env` file in the project root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# twinLab\n",
    "import twinlab as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Campaign\n",
    "CAMPAIGN_ID = \"cosmology\"\n",
    "\n",
    "# Data options\n",
    "POWER_RATIO = False\n",
    "POWER_LOG = True\n",
    "POWER_LATIN = False\n",
    "TRAINING_POINTS = 100\n",
    "SVD_VARIANCE = 0.9999\n",
    "\n",
    "# Data files\n",
    "TRAINING_FILEBASE = \"cosmo\"\n",
    "EVALUATION_FILEBASE = \"eval\"\n",
    "GRID_DATA = \"grid.csv\"\n",
    "\n",
    "# Directories\n",
    "CAMPAIGN_DIR = \"./resources/campaigns/cosmology\"\n",
    "DATASETS_DIR = \"./resources/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378bdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latin_thing = '_latin' if POWER_LATIN else ''\n",
    "ratio_thing = '_ratio' if POWER_RATIO else ''\n",
    "log_thing = '_log' if POWER_LOG else ''\n",
    "\n",
    "TRAINING_DATA = TRAINING_FILEBASE+latin_thing+ratio_thing+log_thing+\".csv\"\n",
    "EVALUATION_DATA = EVALUATION_FILEBASE+ratio_thing+log_thing+\".csv\"\n",
    "\n",
    "# File paths\n",
    "DATASET_PATH = os.path.join(DATASETS_DIR, TRAINING_DATA)\n",
    "EVALUATION_PATH = os.path.join(CAMPAIGN_DIR, EVALUATION_DATA)\n",
    "GRID_PATH = os.path.join(CAMPAIGN_DIR, GRID_DATA)\n",
    "\n",
    "# Write to screen\n",
    "print(f\"Grid........ {GRID_PATH}\")\n",
    "print(f\"Dataset..... {DATASET_PATH}\")\n",
    "print(f\"Evaluate.... {EVALUATION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c5ffa",
   "metadata": {},
   "source": [
    "Ensure that the correct group and user names are reported.\n",
    "These are used to track client usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996ed45",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729e31dc",
   "metadata": {},
   "source": [
    "### Upload dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8fca977",
   "metadata": {},
   "source": [
    "We'll use the csv data in `resources/datasets/cosmology.csv` to train our emulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a059f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.upload_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b7be7",
   "metadata": {},
   "source": [
    "### List datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff52fa",
   "metadata": {},
   "source": [
    "Check which datasets are avalible to train with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tl.list_datasets()\n",
    "pprint(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b255b8",
   "metadata": {},
   "source": [
    "### View dataset statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9973bdd",
   "metadata": {},
   "source": [
    "You can query the dataset to provide a statistical summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1dc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tl.query_dataset(TRAINING_DATA)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "020843c1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d02ebd9a",
   "metadata": {},
   "source": [
    "Set the emulator training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "nk = 100\n",
    "cosmological_parameters = [\"Omega_c\", \"Omega_b\", \"h\", \"ns\", \"sigma_8\"]\n",
    "wavenumber_columns = [f\"k{i}\" for i in range(nk)]\n",
    "params = {\n",
    "    \"filename\": TRAINING_DATA,\n",
    "    \"inputs\": cosmological_parameters,\n",
    "    \"outputs\": wavenumber_columns,\n",
    "    \"decompose_outputs\": True,\n",
    "    \"output_explained_variance\": SVD_VARIANCE,\n",
    "    \"train_test_split\": TRAINING_POINTS,\n",
    "}\n",
    "pprint(params, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b083f8",
   "metadata": {},
   "source": [
    "Train the emulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.train_campaign(params, CAMPAIGN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b9857",
   "metadata": {},
   "source": [
    "Check which campaigns are ready with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "campaigns = tl.list_campaigns()\n",
    "pprint(campaigns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f359f0d9",
   "metadata": {},
   "source": [
    "View the metadata of an emulator with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3911b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tl.query_campaign(CAMPAIGN_ID)\n",
    "pprint(query, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb1bc7",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "106a9c99",
   "metadata": {},
   "source": [
    "Sample the trained emulator with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_mean, df_eval_std = tl.sample_campaign(EVALUATION_PATH, CAMPAIGN_ID)\n",
    "df_train_mean, df_train_std = tl.sample_campaign(DATASET_PATH, CAMPAIGN_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c97b226",
   "metadata": {},
   "source": [
    "Read in the evaluation data and the grid of $k$ values on which to evaluate $P(k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATASET_PATH)\n",
    "df_grid = pd.read_csv(GRID_PATH)\n",
    "df_eval = pd.read_csv(EVALUATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee83b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy statistics\n",
    "if POWER_LOG: # Note that the difference is appropriate here because the data may be log\n",
    "    accuracy_eval = (df_eval_mean[wavenumber_columns]-df_eval[wavenumber_columns]).std()\n",
    "    accuracy_train = (df_train_mean[wavenumber_columns]-df_train[wavenumber_columns]).std()\n",
    "else:\n",
    "    accuracy_eval = (df_eval_mean[wavenumber_columns]/df_eval[wavenumber_columns]-1.).std()\n",
    "    accuracy_train = (df_train_mean[wavenumber_columns]/df_train[wavenumber_columns]-1.).std()\n",
    "\n",
    "plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "# Accuracy of model\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df_grid.iloc[0].values, 100.*accuracy_eval, label='Evaluation data')\n",
    "plt.plot(df_grid.iloc[0].values, 100.*accuracy_train, label='Training data')\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "plt.ylim(bottom=0.)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy of preicted error\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df_grid.iloc[0].values, accuracy_eval/df_eval_std.iloc[0].values)\n",
    "plt.plot(df_grid.iloc[0].values, accuracy_train/df_train_std.iloc[0].values)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"Fourier wavenumber [$h$ Mpc$^{-1}$]\")\n",
    "plt.axhline(1., color=\"k\", linestyle=\":\", lw=0.5)\n",
    "plt.ylabel(\"Relative accuracy\")\n",
    "plt.ylim((0., 2.))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff61305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "nsig = [1, 2]  # sigma errors to plot\n",
    "mfac = 10.  # Factor to inflate error bars by\n",
    "alpha_data = 0.5\n",
    "alpha_model = 0.5\n",
    "color_model = \"C0\"\n",
    "plot_band = True\n",
    "plot_mean = True\n",
    "npow = 5\n",
    "\n",
    "print(f\"Error magnification factor: {mfac} \\n\")\n",
    "\n",
    "# Plot power\n",
    "plt.subplots(1, npow, figsize=(npow*3.5, 3.), sharex=True, sharey=True)\n",
    "grid = df_grid.iloc[0].values\n",
    "for i in range(npow):\n",
    "    plt.subplot(1, npow, i+1)\n",
    "    if POWER_RATIO:\n",
    "        plt.axhline(1., color=\"black\", lw=0.5)\n",
    "    eval = df_eval[wavenumber_columns].iloc[i].values\n",
    "    mean = df_eval_mean.iloc[i].values\n",
    "    err = df_eval_std.iloc[i].values\n",
    "    if POWER_LOG:\n",
    "        eval, mean = np.exp(eval), np.exp(mean)\n",
    "    label = \"Evaluation data\" if i==0 else None\n",
    "    plt.plot(grid, eval, color=\"black\", alpha=alpha_data, label=label)\n",
    "    if plot_band:\n",
    "        for sig in nsig:\n",
    "            if POWER_LOG:\n",
    "                ymin, ymax = np.exp(-mfac*sig*err), np.exp(mfac*sig*err)\n",
    "                ymin, ymax = mean*ymin, mean*ymax\n",
    "            else:\n",
    "                ymin, ymax = -mfac*sig*err, mfac*sig*err\n",
    "                ymin, ymax = mean+ymin, mean+ymax\n",
    "            label = \"Model\" if sig==nsig[0] else None\n",
    "            plt.fill_between(grid, ymin, ymax, color=color_model, lw=0., alpha=alpha_model/sig, label=label)\n",
    "    if plot_mean:\n",
    "        label  = \"Model\" if not plot_band else None\n",
    "        plt.plot(grid, mean, color=color_model, alpha=alpha_model, label=label)\n",
    "    plt.xlabel(r\"Fourier wavenumber [$h$ Mpc$^{-1}$]\")\n",
    "    plt.xscale(\"log\")\n",
    "    if i==0: \n",
    "        if POWER_RATIO:\n",
    "            plt.ylabel(r\"Power ratio with linear\")\n",
    "        else:\n",
    "            plt.ylabel(r\"Power spectrum [$(h^{-1}\\mathrm{Mpc})^3$]\")\n",
    "        plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nsig = [1, 2]\n",
    "alpha_data = 0.5\n",
    "alpha_model = 0.5\n",
    "color_model = \"C0\"\n",
    "plot_train = True\n",
    "alpha_train = 0.5\n",
    "color_train = \"C1\"\n",
    "dr = 3.\n",
    "plot_band = True\n",
    "plot_mean = True\n",
    "ncos = 25\n",
    "nrow = 5\n",
    "\n",
    "# Calculations\n",
    "ncol = ncos//nrow\n",
    "\n",
    "# Plot\n",
    "grid = df_grid.iloc[0].values\n",
    "plt.subplots(nrow, ncol, figsize=(3*ncol, 2.5*nrow), sharex=True, sharey=True)\n",
    "for i in range(ncos):\n",
    "    plt.axhline(0., color=\"black\", lw=1)\n",
    "    plt.subplot(nrow, ncol, i+1)\n",
    "    eval = df_eval[wavenumber_columns].iloc[i].values\n",
    "    eval_mean = df_eval_mean.iloc[i].values\n",
    "    eval_err = df_eval_std.iloc[i].values\n",
    "    train = df_train[wavenumber_columns].iloc[i].values\n",
    "    train_mean = df_train_mean.iloc[i].values\n",
    "    train_err = df_train_std.iloc[i].values\n",
    "    if POWER_LOG:\n",
    "        eval, eval_mean = np.exp(eval), np.exp(eval_mean)\n",
    "        train, train_mean = np.exp(train), np.exp(train_mean)\n",
    "    if plot_band:\n",
    "        for sig in nsig:\n",
    "            if POWER_LOG:\n",
    "                ymin, ymax = np.exp(-sig*eval_err), np.exp(sig*eval_err)\n",
    "                ymin, ymax = 100.*((eval_mean*ymin)/eval-1.), 100.*((eval_mean*ymax)/eval-1.)\n",
    "            else:\n",
    "                ymin, ymax = -sig*eval_err, sig*eval_err\n",
    "                ymin, ymax = 100.*((eval_mean+ymin)/eval-1.), 100.*((eval_mean+ymax)/eval-1.)\n",
    "            label = \"Model on evaluation data\" if sig==nsig[0] else None\n",
    "            plt.fill_between(grid, ymin, ymax, color=color_model, lw=0, alpha=alpha_model/sig, label=label)\n",
    "            if plot_train:\n",
    "                if POWER_LOG:\n",
    "                    ymin, ymax = np.exp(-sig*train_err), np.exp(sig*train_err)\n",
    "                    ymin, ymax = 100.*((train_mean*ymin)/train-1.), 100.*((train_mean*ymax)/train-1.)\n",
    "                else:\n",
    "                    ymin, ymax = -sig*train_err, sig*train_err\n",
    "                    ymin, ymax = 100.*((train_mean+ymin)/train-1.), 100.*((train_mean+ymax)/train-1.)\n",
    "                label = \"Model on training data\" if sig==nsig[0] else None\n",
    "                plt.fill_between(grid, ymin, ymax, color=color_train, lw=0, alpha=alpha_train/sig, label=label)\n",
    "    if plot_mean:\n",
    "        label = \"Model on evaluation data\" if not plot_band else None\n",
    "        y = 100.*(eval_mean/eval-1.)\n",
    "        plt.plot(grid, y, color=color_model, alpha=alpha_model, label=label)\n",
    "        if plot_train:\n",
    "            label = \"Model on training data\" if not plot_band else None\n",
    "            y = 100.*(train_mean/train-1.)\n",
    "            plt.plot(grid, y, color=color_train, alpha=alpha_train, label=label)\n",
    "    if i//ncol==nrow-1: plt.xlabel(r\"$k/h\\mathrm{Mpc}^{-1}$\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlim((grid[0], grid[-1]))\n",
    "    if i%ncol==0: plt.ylabel(r\"$P_\\mathrm{model}(k)/P_\\mathrm{truth}(k)-1$ [%]\")\n",
    "    plt.ylim((-dr, dr))\n",
    "    if i==0: plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc41b52",
   "metadata": {},
   "source": [
    "Finally, delete a trained emulator and the dataset if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.delete_campaign(CAMPAIGN_ID)\n",
    "tl.delete_dataset(TRAINING_DATA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
